<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Holly Dinkel</title> <meta name="author" content="Holly Dinkel"> <meta name="description" content="Academic website and portfolio of Holly Dinkel. "> <meta name="keywords" content="robotics, computer vision, space"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/astrobee.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hollydinkel.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Holly <span class="font-weight-bold">Dinkel</span> </h1> <p class="desc">Ph.D. Candidate in Aerospace Engineering</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg?7adf2ebd89273312738d35b67e030005" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Urbana, Illinois. 61801</p> </div> </div> <div class="clearfix"> <p>Holly Dinkel is pursuing the Ph.D. in aerospace engineering at the University of Illinois Urbana-Champaign where she researches robotic caretaking as a NASA Space Technology Graduate Research Fellow with the NASA Ames Research Center Intelligent Robotics Group and the NASA Johnson Space Center Dexterous Robotics Laboratory. She previously completed the M.S. in aeronautics and astronautics and a certificate in entrepreneurship at Stanford University and the B.S. and B.A. in chemical engineering and music, respectively, at the University of Missouri, Columbia.</p> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Oct 1, 2023</th> <td> <a href="https://aeroastro.mit.edu/future-leaders-in-aerospace/schedule-at-a-glance/" rel="external nofollow noopener" target="_blank">Dinkel among 40 participants in MIT Aero/Astro Future Leaders in Aerospace Symposium</a> </td> </tr> <tr> <th scope="row">Aug 9, 2023</th> <td> <a href="https://aerospace.illinois.edu/news/57521" rel="external nofollow noopener" target="_blank">Illinois team wins first prize in OpenCV AI competition</a> </td> </tr> <tr> <th scope="row">Jun 5, 2023</th> <td> <a href="https://aerospace.illinois.edu/news/56510" rel="external nofollow noopener" target="_blank">Dinkel receives 2023 Zonta International Amelia Earhart Fellowship</a> </td> </tr> <tr> <th scope="row">Apr 12, 2023</th> <td> <a href="https://aerospace.illinois.edu/news/54710" rel="external nofollow noopener" target="_blank">Holly Dinkel receives P.E.O. Scholar Award</a> </td> </tr> <tr> <th scope="row">Nov 17, 2021</th> <td> <a href="https://usrussia.stanford.edu/surf-news/2020-2021-space-cooperation-fellows-present-surf-research-at-the-global-space-exploration-conference-in-russianbsp" rel="external nofollow noopener" target="_blank">2020-2021 Space Cooperation Fellows Present SURF Research at the Global Space Exploration Conference in Russia</a> </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Publications</a></h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/iac2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/iac2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/iac2024-1400.webp"></source> <img src="/assets/img/publication_preview/iac2024.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="iac2024.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cambone2024business" class="col-sm-8"> <div class="title">Business Innovation in Commercial Space: Culture and Trends in Earth Observation</div> <div class="author"> <a href="https://www.linkedin.com/in/giulia-cambone-075723210" rel="external nofollow noopener" target="_blank">Giulia Cambone</a>,Â <em><strong>Holly Dinkel</strong></em>,Â <a href="https://uk.linkedin.com/in/luca-ferrone" rel="external nofollow noopener" target="_blank">Luca Ferrone</a>,Â <a href="https://sg.linkedin.com/in/antoniofowlstark" rel="external nofollow noopener" target="_blank">KangSan Kim</a>,Â <a href="https://www.linkedin.com/in/shinsukekito/" rel="external nofollow noopener" target="_blank">Shinsuke Kito</a>,Â andÂ <a href="https://www.linkedin.com/in/chawalwat-martkamjan/" rel="external nofollow noopener" target="_blank">Chawalwat Martkamjan</a> </div> <div class="periodical"> <em>In IAF International Astronautical Congress</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/IAC2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/hollydinkel/space_econometrics" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This work studies technological, business, and cultural development practices in the commercial Earth Observation (EO) sector. Increased access to space combined with growing demands for climate and natural resource monitoring, situational awareness, on-demand analytics, and sovereign data independence motivate the expansion of EO. The financial performance and culture of five public EO companiesâ€“iQPS, GOMSpace, Kleos Space, Planet Labs, and Satellogicâ€“are analyzed. Time-series econometrics modeling indicates trends in how financial resources impact earnings and revenue, and the models are used to forecast future earnings and revenue with a high goodness-of-fit. A time-series DuPont analysis investigates how companies operate through conversion of different financial instruments, and corresponding corporate milestones contextualize these financial transactions. Informational interviews of four EO company executives provide clues about company culture and decision-making strategies. Recommendations tailored to the unique challenges and opportunities of the global EO sector are provided to help government partners, investors, and program managers estimate business performance given financial and cultural data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/astrobee-iss.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/astrobee-iss.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/astrobee-iss.gif-1400.webp"></source> <img src="/assets/img/publication_preview/astrobee-iss.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="astrobee-iss.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dinkel2024astrobeecd" class="col-sm-8"> <div class="title">AstrobeeCD: Change Detection in Microgravity with Free-Flying Robots</div> <div class="author"> <em><strong>Holly Dinkel</strong></em>,Â <a href="https://web.stanford.edu/~juliadi/" rel="external nofollow noopener" target="_blank">Julia Di</a>,Â <a href="https://www.linkedin.com/in/jamiecsantos/" rel="external nofollow noopener" target="_blank">Jamie Santos</a>,Â <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/keenan-albee/" rel="external nofollow noopener" target="_blank">Keenan Albee</a>,Â <a href="www.paulovinicius.com">Paulo V.K. Borges</a>,Â <a href="https://www.linkedin.com/in/marinagmoreira" rel="external nofollow noopener" target="_blank">Marina Moreira</a>,Â <a href="https://www.linkedin.com/in/ryansoussan/" rel="external nofollow noopener" target="_blank">Ryan Soussan</a>,Â <a href="https://linkedin.com/in/olegalexandrov" rel="external nofollow noopener" target="_blank">Oleg Alexandrov</a>,Â <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>,Â andÂ <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a> </div> <div class="periodical"> <em>Acta Astronautica</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://www.sciencedirect.com/science/article/pii/S0094576524003539" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/hollydinkel/astrobeecd" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=VfjV-zwFEtU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1016/j.actaastro.2024.06.037"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1016/j.actaastro.2024.06.037" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This work presents AstrobeeCD, a system for 3D scene change detection toward near-real-time environmental awareness of space outposts using the Astrobee free-flying robot in microgravity. Assistive free-flyer robots autonomously caring for future crewed space habitats must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. A set of image and depth data from one time step is used to reconstruct a 3D model of the environment. The 3D model is used as the basis for comparison for free-flyer environment surveys at future time steps, where an image-based change detection algorithm identifies inconsistencies against the 3D model. Change detection is demonstrated using real image and pose data collected by an Astrobee robot in a test environment on Earth at NASA Ames Research Center and from microgravity aboard the International Space Station. Change detection computation time and performance are quantitatively evaluated on the test data captured on Earth, and it identifies scene changes more quickly than a point cloud clustering-based algorithm applied to data from the same surveys.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/knotdlo.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/knotdlo.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/knotdlo.gif-1400.webp"></source> <img src="/assets/img/publication_preview/knotdlo.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="knotdlo.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dinkel2024knotdlo" class="col-sm-8"> <div class="title">KnotDLO: Toward Interpretable Knot Tying</div> <div class="author"> <em><strong>Holly Dinkel</strong></em>,Â <a href="https://www.rsnavaratna.com/" rel="external nofollow noopener" target="_blank">Raghavendra Navaratna</a>,Â <a href="https://jingyi-xiang.github.io/" rel="external nofollow noopener" target="_blank">Jingyi Xiang</a>,Â <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>,Â <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a>,Â andÂ <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Timothy Bretl</a> </div> <div class="periodical"> <em>In IEEE ICRA Workshop on 3D Visual Representations for Manipulation</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://openreview.net/pdf?id=vsaEOFOUyY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/ICRA20243DVRM_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://youtu.be/mg30uCUtpOk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This work presents KnotDLO, a method for one-handed knot manipulation to transform a Deformable Linear Object (DLO) into knot states. Grasp and target waypoints for future DLO states are planned from the current DLO shape. Grasp poses are computed from indexing the tracked piecewise linear curve representing the DLO state based on the current curve shape and are piecewise continuous. Intermediate waypoints are computed from the geometry of the current DLO state and the desired next state. The perception and manipulation system is robust to occlusion, repeatable for varying rope initial configurations, interpretable for generating motion policies, and requires no human demonstrations or training. The system decouples visual reasoning from control. In 16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an overhand knot from previously unseen configurations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/trackdlo.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/trackdlo.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/trackdlo.gif-1400.webp"></source> <img src="/assets/img/publication_preview/trackdlo.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="trackdlo.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xiang2024trackdlo" class="col-sm-8"> <div class="title">TrackDLO: Tracking Deformable Linear Objects Under Occlusion with Motion Coherence</div> <div class="author"> <a href="https://jingyi-xiang.github.io/" rel="external nofollow noopener" target="_blank">Jingyi Xiang</a>,Â <em><strong>Holly Dinkel</strong></em>,Â <a href="https://www.linkedin.com/in/zhao-harry/" rel="external nofollow noopener" target="_blank">Harry Zhao</a>,Â <a href="https://www.linkedin.com/in/naixiang-gao-543348272/" rel="external nofollow noopener" target="_blank">Naixiang Gao</a>,Â <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>,Â <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a>,Â andÂ <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Timothy Bretl</a> </div> <div class="periodical"> <em>IEEE ICRA Workshop on Representing and Manipulating Deformable Objects</em>, May 2024 </div> <div class="periodical"> </div> <div class="award"> <strong>Best Abstract Finalist ğŸ†</strong> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://deformable-workshop.github.io/icra2024/spotlight/02_07_wdo_xiang_trackdlo.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/RMDLO/trackdlo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ICRA2024RMDO_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.youtube.com/watch?v=OOgPZOQYRc4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The TrackDLO algorithm estimates the shape of a Deformable Linear Object (DLO) under occlusion from a sequence of RGB-D images. TrackDLO is vision-only and runs in real-time. It requires no external state information from physics modeling, simulation, visual markers, or contact as input. The algorithm improves on previous approaches by addressing three common scenarios which cause tracking failure: tip occlusion, mid-section occlusion, and self-occlusion. This is achieved through the application of Motion Coherence Theory to impute the spatial velocity of occluded nodes, the use of the topological geodesic distance to track self-occluding DLOs, and the introduction of a non-Gaussian kernel that only penalizes lower-order spatial displacement derivatives to reflect DLO physics. Improved real-time DLO tracking under mid-section occlusion, tip occlusion, and self-occlusion is demonstrated experimentally. The source code and demonstration data are publicly released.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/astrobee-granite.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/astrobee-granite.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/astrobee-granite.gif-1400.webp"></source> <img src="/assets/img/publication_preview/astrobee-granite.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="astrobee-granite.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="santos2024aiaa" class="col-sm-8"> <div class="title">Unsupervised Change Detection for Space Habitats Using 3D Point Clouds</div> <div class="author"> <a href="https://www.linkedin.com/in/jamiecsantos/" rel="external nofollow noopener" target="_blank">Jamie Santos</a>,Â <em><strong>Holly Dinkel</strong></em>,Â <a href="https://web.stanford.edu/~juliadi/" rel="external nofollow noopener" target="_blank">Julia Di</a>,Â <a href="www.paulovinicius.com">Paulo V.K. Borges</a>,Â <a href="https://www.linkedin.com/in/marinagmoreira" rel="external nofollow noopener" target="_blank">Marina Moreira</a>,Â <a href="https://linkedin.com/in/olegalexandrov" rel="external nofollow noopener" target="_blank">Oleg Alexandrov</a>,Â <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>,Â andÂ <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a> </div> <div class="periodical"> <em>In AIAA SciTech Forum</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arc.aiaa.org/doi/10.2514/6.2024-1960" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=7WHp0dQYG4Y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.2514/6.2024-1960"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.2514/6.2024-1960" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This work presents an algorithm for scene change detection from point clouds to enable autonomous robotic caretaking in future space habitats. Autonomous robotic systems will help maintain future deep-space habitats, such as the Gateway space station, which will be uncrewed for extended periods. Existing scene analysis software used on the International Space Station (ISS) relies on manually-labeled images for detecting changes. In contrast, the algorithm presented in this work uses raw, unlabeled point clouds as inputs. The algorithm first applies modified Expectation-Maximization Gaussian Mixture Model (GMM) clustering to two input point clouds. It then performs change detection by comparing the GMMs using the Earth Moverâ€™s Distance. The algorithm is validated quantitatively and qualitatively using a test dataset collected by an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth images taken directly by Astrobee and full-scene reconstructed maps built with RGB-D and pose data from Astrobee. The runtimes of the approach are also analyzed in depth. The source code is publicly released to promote further development.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/real2sim_comparison.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/real2sim_comparison.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/real2sim_comparison.gif-1400.webp"></source> <img src="/assets/img/publication_preview/real2sim_comparison.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="real2sim_comparison.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ta2023sim" class="col-sm-8"> <div class="title">The Impact of Time Step Frequency on the Realism of Robotic Manipulation Simulation for Objects of Different Scales</div> <div class="author"> <a href="https://www.linkedin.com/in/quang-minh-ta/" rel="external nofollow noopener" target="_blank">Minh Quang Ta*</a>,Â <em><strong>Holly Dinkel*</strong></em>,Â <a href="https://hammania689.github.io/" rel="external nofollow noopener" target="_blank">Hameed Abdul-Rashid</a>,Â <a href="https://yangfei4.github.io/" rel="external nofollow noopener" target="_blank">Yangfei Dai</a>,Â <a href="https://www.linkedin.com/in/jessica-myers-552252191" rel="external nofollow noopener" target="_blank">Jessica Myers</a>,Â <a href="https://chentan.github.io/" rel="external nofollow noopener" target="_blank">Tan Chen</a>,Â <a href="https://ari-psu.github.io/" rel="external nofollow noopener" target="_blank">Junyi Geng</a>,Â andÂ <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Timothy Bretl</a> </div> <div class="periodical"> <em>In IEEE/RSJ IROS Workshop on Robotics and AI in Future Factory (RAFF)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="award"> <strong>Best Poster Finalist ğŸ†</strong> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2310.08233.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/IROS2023_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.youtube.com/watch?v=JOXrBpMmI0A" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This work evaluates the impact of time step frequency and component scale on robotic manipulation simulation accuracy. Increasing the time step frequency for small-scale objects is shown to improve simulation accuracy. This simulation, demonstrating pre-assembly part picking for two object geometries, serves as a starting point for discussing how to improve Sim2Real transfer in robotic assembly processes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/trackdlo.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/trackdlo.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/trackdlo.gif-1400.webp"></source> <img src="/assets/img/publication_preview/trackdlo.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="trackdlo.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xiang2023trackdlo" class="col-sm-8"> <div class="title">TrackDLO: Tracking Deformable Linear Objects Under Occlusion with Motion Coherence</div> <div class="author"> <a href="https://jingyi-xiang.github.io/" rel="external nofollow noopener" target="_blank">Jingyi Xiang</a>,Â <em><strong>Holly Dinkel</strong></em>,Â <a href="https://www.linkedin.com/in/zhao-harry/" rel="external nofollow noopener" target="_blank">Harry Zhao</a>,Â <a href="https://www.linkedin.com/in/naixiang-gao-543348272/" rel="external nofollow noopener" target="_blank">Naixiang Gao</a>,Â <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>,Â <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a>,Â andÂ <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Timothy Bretl</a> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://ieeexplore.ieee.org/document/10214157" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/RMDLO/trackdlo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ICRA2024RMDO_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://youtu.be/MxqNJsen5eg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1109/LRA.2023.3303710"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1109/LRA.2023.3303710" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The TrackDLO algorithm estimates the shape of a Deformable Linear Object (DLO) under occlusion from a sequence of RGB-D images. TrackDLO is vision-only and runs in real-time. It requires no external state information from physics modeling, simulation, visual markers, or contact as input. The algorithm improves on previous approaches by addressing three common scenarios which cause tracking failure: tip occlusion, mid-section occlusion, and self-occlusion. This is achieved through the application of Motion Coherence Theory to impute the spatial velocity of occluded nodes, the use of the topological geodesic distance to track self-occluding DLOs, and the introduction of a non-Gaussian kernel that only penalizes lower-order spatial displacement derivatives to reflect DLO physics. Improved real-time DLO tracking under mid-section occlusion, tip occlusion, and self-occlusion is demonstrated experimentally. The source code and demonstration data are publicly released.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/multidlo.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/multidlo.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/multidlo.gif-1400.webp"></source> <img src="/assets/img/publication_preview/multidlo.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="multidlo.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xiang2023multidlo" class="col-sm-8"> <div class="title">Simultaneous Shape Tracking of Multiple Deformable Linear Objects with Global-Local Topology Preservation</div> <div class="author"> <a href="https://jingyi-xiang.github.io/" rel="external nofollow noopener" target="_blank">Jingyi Xiang</a>,Â andÂ <em><strong>Holly Dinkel</strong></em> </div> <div class="periodical"> <em>In IEEE ICRA Workshop on Representing and Manipulating Deformable Objects</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2310.13245.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/RMDLO/multi-dlo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ICRA2023RMDO_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://youtu.be/hfiqwMxitqA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This work presents an algorithm for tracking the shape of multiple entangling Deformable Linear Objects (DLOs) from a sequence of RGB-D images. This algorithm runs in real-time and improves on previous single-DLO tracking approaches by enabling tracking of multiple objects. This is achieved using Global-Local Topology Preservation (GLTP). This work uses the geodesic distance in GLTP to define the distance between separate objects and the distance between different parts of the same object. Tracking multiple entangling DLOs is demonstrated experimentally. The source code is publicly released. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/iac2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/iac2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/iac2023-1400.webp"></source> <img src="/assets/img/publication_preview/iac2023.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="iac2023.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dinkel2023iac" class="col-sm-8"> <div class="title">Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots</div> <div class="author"> <em><strong>Holly Dinkel*</strong></em>,Â <a href="https://web.stanford.edu/~juliadi/" rel="external nofollow noopener" target="_blank">Julia Di*</a>,Â <a href="https://www.linkedin.com/in/jamiecsantos/" rel="external nofollow noopener" target="_blank">Jamie Santos</a>,Â <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/keenan-albee/" rel="external nofollow noopener" target="_blank">Keenan Albee</a>,Â <a href="www.paulovinicius.com">Paulo V.K. Borges</a>,Â <a href="https://www.linkedin.com/in/marinagmoreira" rel="external nofollow noopener" target="_blank">Marina Moreira</a>,Â <a href="https://linkedin.com/in/olegalexandrov" rel="external nofollow noopener" target="_blank">Oleg Alexandrov</a>,Â <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>,Â andÂ <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a> </div> <div class="periodical"> <em>In IAF International Astronautical Congress</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2311.02558.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/hollydinkel/astrobeecd" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.youtube.com/watch?v=VfjV-zwFEtU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Assistive free-flyer robots autonomously caring for future crewed outpostsâ€”such as NASAâ€™s Astrobee robots on the International Space Station (ISS)â€”must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. This work presents a framework for multi-agent cooperative mapping and change detection to enable robotic maintenance of space outposts. One agent is used to reconstruct a 3D model of the environment from sequences of images and corresponding depth information. Another agent is used to periodically scan the environment for inconsistencies against the 3D model. Change detection is validated \textita posteriori using real image and pose data collected by Astrobee robots in a ground testbed environment and from microgravity aboard the ISS. This work outlines the objectives, requirements, and algorithmic modules for the multi-agent reconstruction system, including recommendations for its use by assistive free-flyers aboard future microgravity outposts.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/lvs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/lvs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/lvs-1400.webp"></source> <img src="/assets/img/publication_preview/lvs.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="lvs.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dinkel2022iac" class="col-sm-8"> <div class="title">Vela: A Data-Driven Proposal for Joint Collaboration in Space Exploration</div> <div class="author"> <em><strong>Holly Dinkel*</strong></em>,Â andÂ <a href="https://www.linkedin.com/in/jason-k-cornelius" rel="external nofollow noopener" target="_blank">Jason Cornelius*</a> </div> <div class="periodical"> <em>In IAF International Astronautical Congress</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2408.04730" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/IAC2022_presentation.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The UN Office of Outer Space Affairs identifies synergy of space development activities and international cooperation through data and infrastructure sharing in their Sustainable Development Goal 17 (SDG17). Current multilateral space exploration paradigms, however, are divided between the Artemis and the Roscosmos-CNSA programs to return to the moon and establish permanent human settlements. As space agencies work to expand human presence in space, economic resource consolidation in pursuit of technologically ambitious space expeditions is the most sensible path to accomplish SDG17. This paper compiles a budget dataset for the top five federally-funded space agencies: CNSA, ESA, JAXA, NASA, and Roscosmos. Using time-series econometric anslysis methods in STATA, this work analyzes each agencyâ€™s economic contributions toward space exploration. The dataset results are used to propose a multinational space mission, Vela, for the development of an orbiting space station around Mars in the late 2030s. Distribution of economic resources and technological capabilities by the respective space programs are proposed to ensure programmatic redundancy and increase the odds of success on the given timeline.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/wire_segmentation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/wire_segmentation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/wire_segmentation-1400.webp"></source> <img src="/assets/img/publication_preview/wire_segmentation.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="wire_segmentation.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dinkel2022RMDOO" class="col-sm-8"> <div class="title">Wire Point Cloud Instance Segmentation from RGBD Imagery with Mask R-CNN</div> <div class="author"> <em><strong>Holly Dinkel*</strong></em>,Â <a href="https://jingyi-xiang.github.io/" rel="external nofollow noopener" target="_blank">Jingyi Xiang*</a>,Â <a href="https://www.linkedin.com/in/zhao-harry/" rel="external nofollow noopener" target="_blank">Harry Zhao</a>,Â <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>,Â <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a>,Â andÂ <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Timothy Bretl</a> </div> <div class="periodical"> <em>In IEEE ICRA Workshop on Representing and Manipulating Deformable Objects</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://deformable-workshop.github.io/icra2022/spotlight/WDOICRA2022_08.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/eqgZQckCDOY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Perception of the shapes of deforming objects like wires enables their monitoring and manipulation by autonomous robots. This paper presents detection, classification, and instance segmentation of deformable wires from a cluttered scene in RGBD imagery. This work uses the Detectron2 implementation of Mask R-CNN trained with the PointRend mask head on the UIUCWires dataset as the framework for wire instance segmentation on RGB imagery, a method demonstrated to perform well for the instance segmentation task in previous work. In this work, the instance bitmask is directly used to segment individual object point clouds, an important step toward wire shape representation for manipulation tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/wotf-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/wotf-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/wotf-1400.webp"></source> <img src="/assets/img/publication_preview/wotf.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="wotf.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2022insights" class="col-sm-8"> <div class="title">Insights from an Industrial Collaborative Assembly Project: Lessons in Research and Collaboration</div> <div class="author"> <a href="https://chentan.github.io/" rel="external nofollow noopener" target="_blank">Tan Chen</a>,Â <a href="https://tedhuang96.github.io/" rel="external nofollow noopener" target="_blank">Zhe Huang</a>,Â <a href="https://parasollab.web.illinois.edu/people/jmotes2/" rel="external nofollow noopener" target="_blank">James Motes</a>,Â <a href="https://ari-psu.github.io/" rel="external nofollow noopener" target="_blank">Junyi Geng</a>,Â <a href="https://www.linkedin.com/in/quang-minh-ta/" rel="external nofollow noopener" target="_blank">Quang Minh Ta</a>,Â <em><strong>Holly Dinkel</strong></em>,Â <a href="https://hammania689.github.io/" rel="external nofollow noopener" target="_blank">Hameed Abdul-Rashid</a>,Â <a href="https://www.linkedin.com/in/jessica-myers-552252191" rel="external nofollow noopener" target="_blank">Jessica Myers</a>,Â <a href="https://yejimun.github.io/" rel="external nofollow noopener" target="_blank">Ye-Ji Mun</a>,Â <a href="https://www.linkedin.com/in/wei-che-lin-778aa6156/" rel="external nofollow noopener" target="_blank">Wei-Che Lin</a>,Â Yuan-Yung Huang,Â <a href="https://www.linkedin.com/in/sizhe-liu/" rel="external nofollow noopener" target="_blank">Sizhe Liu</a>,Â <a href="https://parasollab.web.illinois.edu/people/moralesa/" rel="external nofollow noopener" target="_blank">Marco Morales</a>,Â <a href="https://parasollab.web.illinois.edu/people/amato/" rel="external nofollow noopener" target="_blank">Nancy Amato</a>,Â <a href="https://krdc.web.illinois.edu/" rel="external nofollow noopener" target="_blank">Katherine Driggs-Campbell</a>,Â andÂ <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Timothy Bretl</a> </div> <div class="periodical"> <em>In IEEE ICRA Workshop on Collaborative Robots and Work of the Future</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2205.14340.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://drive.google.com/file/d/1Vce93zH5N1a0dci7OP_YU3OjusJUVI_n/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Significant progress in robotics reveals new opportunities to advance manufacturing. Next-generation industrial automation will require both integration of distinct robotic technologies and their application to challenging industrial environments. This paper presents lessons from a collaborative assembly project between three academic research groups and an industry partner. The goal of the project is to develop a flexible, safe, and productive manufacturing cell for sub-centimeter precision assembly. Solving this problem in a high-mix, low-volume production line motivates multiple research thrusts in robotics. This work identifies new directions in collaborative robotics for industrial applications and offers insight toward strengthening collaborations between institutions in academia and industry on the development of new technologies.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/LaunchPlot.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/LaunchPlot.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/LaunchPlot.PNG-1400.webp"></source> <img src="/assets/img/publication_preview/LaunchPlot.PNG" class="preview z-depth-1 rounded" width="auto" height="auto" alt="LaunchPlot.PNG" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cornelius2021glex" class="col-sm-8"> <div class="title">Development of a Private Space Sector in the U.S. and Russia</div> <div class="author"> <a href="https://www.linkedin.com/in/jason-k-cornelius" rel="external nofollow noopener" target="_blank">Jason Cornelius</a>,Â <em><strong>Holly Dinkel</strong></em>,Â andÂ <a href="https://www.linkedin.com/in/arzukurgan/" rel="external nofollow noopener" target="_blank">Arzu Kurgan</a> </div> <div class="periodical"> <em>In IAF Global Space Exploration Conference (GLEX)</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="award"> <strong>Best Technical Presentation Finalist ğŸ†</strong> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/GLEX2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=cPIoYJtPwak" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://youtu.be/xt2uH0qKv9s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This work analyzes progress and challenges in the development of commercial space economies in the United States and Russia. Space development progress is characterized through examination of three production indicators for the space economies in each country and econometric analysis of the financial data for three new American private space companies. We also provide case studies of major challenges faced by three new private space companies and conjecture about the future of the private space industry through analysis of space education projects in each country. As leaders of space technological development and exploration, the U.S. and Russia are major stakeholders in the current and future global space economy. We find access to capital is a primary barrier to entrepreneurship. The results will help future founders more effectively navigate the complex financial, regulatory, and technological landscape of the space industry.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/beam_monitor-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/beam_monitor-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/beam_monitor-1400.webp"></source> <img src="/assets/img/publication_preview/beam_monitor.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="beam_monitor.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yonehara2016gas" class="col-sm-8"> <div class="title">Gas Filled RF Resonator Hadron Beam Monitor for Intense Neutrino Beam Experiments</div> <div class="author"> <a href="https://www.linkedin.com/in/katsuya-yonehara-46b14965/" rel="external nofollow noopener" target="_blank">Katsuya Yonehara</a>,Â <a href="https://www.linkedin.com/in/robert-abrams-146a8823/" rel="external nofollow noopener" target="_blank">Robert Abrams</a>,Â <em><strong>Holly Dinkel</strong></em>,Â <a href="https://www.linkedin.com/in/ben-freemire-10506512" rel="external nofollow noopener" target="_blank">Ben Freemire</a>,Â <a href="https://www.linkedin.com/in/rolland-johnson-99266767" rel="external nofollow noopener" target="_blank">Rolland Johnson</a>,Â <a href="https://inspirehep.net/authors/1046265" rel="external nofollow noopener" target="_blank">Grigory Kazakevich</a>,Â <a href="https://en.wikipedia.org/wiki/Alvin_V._Tollestrup" rel="external nofollow noopener" target="_blank">Alvin Tollestrup</a>,Â andÂ <a href="https://www.linkedin.com/in/bob-zwaska/" rel="external nofollow noopener" target="_blank">Robert Zwaska</a> </div> <div class="periodical"> <em>In International Particle Accelerator Conference</em>, Jun 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://www.osti.gov/servlets/purl/1280866" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>MW-class beam facilities are being considered all over the world to produce an intense neutrino beam for fundamental particle physics experiments. A radiation-robust beam monitor system is required to diagnose the primary and secondary beam qualities in high-radiation environments. We have proposed a novel gas-filled RF-resonator hadron beam monitor in which charged particles passing through the resonator produce ionized plasma that changes the permittivity of the gas. The sensitivity of the monitor has been evaluated in numerical simulation. A signal manipulation algorithm has been designed. A prototype system will be constructed and tested by using a proton beam at the MuCool Test Area at Fermilab.</p> </div> </div> </div> </li></ol> * Equal Contribution </div> <h2><a href="#" style="color: inherit;">Viewer Map</a></h2> <hr> <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=lWoAWy0cj1ucRuH-t-uem8T2KNJHfpfwqz4vUTJkSW4&amp;cl=ffffff&amp;w=a"></script> <hr> <div class="social"> <div class="contact-icons"> <a href="mailto:%68%64%69%6E%6B%65%6C%32@%69%6C%6C%69%6E%6F%69%73.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-7510-2066" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=5LW2KOkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/hollydinkel" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/hollydinkel" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://youtube.com/@uiucrmdlo" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fab fa-youtube"></i></a> </div> <div class="contact-note"> If you are interested in collaborating, please send me an email! </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2024 Holly Dinkel. Last updated: September 27, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>